---
title: "MLP、CNN、RNN、LSTM、GRU"
date: 2025-08-22 10:00:00 +0800
categories: [AI, Models] #[Top_category, sub_category]
tags: [ai, models] ## TAG names should always be lowercase
# author: 
---

---

# 神经网络结构总结：MLP、CNN、RNN、LSTM、GRU

---

## 一、MLP（多层感知机）

* **定义**：多层感知机（MLP） = 全连接神经网络（FCN）的子集。
  每个神经元与前一层所有节点相连。

  注：MLP 和 CNN 都属于前馈神经网络forward network。

MLP是多层感知机-全连接神经网络，每个神经元连接所有前层节点，导致训练参数过多，且没有空间或者时序上的关联性，只适用于结构化数据（如：表格）或小型的数据。

CNN则是**局部连接**，因其卷积核convolutional kernel只连接了一部分的输入区域，而一个卷积核的滑过图像让CNN具备有了参数共享这一特点。这种**参数共享**的特性不仅进一步减少了模型参数，还使得网络具备了平移不变性（translation invariance），即无论目标物体出现在图像的哪个位置，网络都能识别它。

* FCN ⊇ MLP

```
前馈神经网络 (Feedforward Neural Network)
├── 多层感知机（MLP）
│   └── 适合结构化数据、基本神经网络构建块
├── 卷积神经网络（CNN）
│   └── 图像与空间任务优选，局部感受野 + 权重共享
└── Vision Transformer（ViT）
    └── 前馈层 + 自注意力 + 残差结构
```

### What are the major advantages of Convolutional Neural Networks (CNNs) over plain Multi-Layer Perceptrons (MLPs) for image tasks?

1.参数效率（CNN的卷积核在空间维度上共享权重，参数量仅随核大小与通道数线性增长。而MLP 需要为每个输入像素都分配独立权重，分辨率一旦提高，参数量呈平方级膨胀。2.局部感受野与平移等变（Locality & Translation Equivariance）
* CNN 的卷积操作天然关注局部邻域；同一过滤器滑窗应用，模型对物体在图中的位置具有位置不敏感特征。
* MLP 把整幅图像一维展开，像素位置被打乱，必须靠数据自己“学习”位移不变性，样本需求更大。


### 1.为什么MLP看上去像是一个弱后的技术，可是在很多新的论文中依然能够频繁的看到它身影？例如GAN，Transformer等等最新的论文中都能看到MLP的结构在其中。1.1 请问MLP在这些结构中主要作用是什么？这些启示给我带来的启发以及更深层次的问题是如何用好MLP，在什么时候能够使用它，以及它的用处是什么？

* **原因**：MLP 作为 “老组件”，在现代架构中依旧承担核心作用

  1. **构建块**：在 Transformer 中，FFN（前馈层）本质就是 MLP
  2. **信息融合**：在 GAN 的判别器/生成器中，MLP 负责高维特征映射
* **启示**：

  * MLP 不是 “过时技术”，而是 **基础组件**
  * 使用场景：

    * 结构化数据建模
    * 作为复杂模型的子模块（GAN判别器、Transformer FFN层）
    * 融合、映射、特征变换

---

## 四、RNN（循环神经网络）

* **定义**：

  * 与 FCN 不同，RNN 在同一层的神经元之间有连接，**共享参数**
  * 输入既传给下一层，也传递给下一个时间步

* **优点**：捕捉时间依赖

* **缺点**：梯度消失/爆炸（链式法则在深层或长序列中放大/缩小梯度）

---

### 1. RNN 梯度消失/爆炸原因

* 反向传播过程中，偏导不断相乘
* 若 <1 → 梯度趋于 0（消失）
* 若 >1 → 梯度趋于 ∞（爆炸）

---

### 2. 常见解决方法

1. 梯度裁剪（Gradient Clipping）
2. 改进优化算法（RMSprop、Adam）
3. 更合适的激活函数（Leaky ReLU、ELU）
4. 改进结构（LSTM、GRU）
5. 引入注意力机制
6. 批标准化（BatchNorm）
7. 调整学习率

---

### 3. RNN 经典结构

* **1对1**：单输入 → 单输出（如情感分类）
* **1对多**：单输入 → 序列输出（图像生成文字）
* **多对1**：序列输入 → 单输出（序列分类）
* **多对多**：

  * N对N：语音识别、时间序列
  * N对M：机器翻译、摘要（encoder-decoder）

---

### 4. RNN 变长输入处理

* **Padding**：统一长度
* **动态填充**：每批次动态选择长度
* **Masking**：忽略填充部分
* **变长RNN（LSTM/GRU）**：天然支持变长输入

---

## 五、LSTM（长短期记忆网络）

* **动机**：解决 RNN 的梯度消失/爆炸问题
* **核心**：门控机制（输入门、遗忘门、输出门）

### 1. 三个门的作用

1. **输入门**：决定当前输入信息多少进入记忆
2. **遗忘门**：决定丢弃多少旧信息
3. **输出门**：决定记忆中哪些部分输出

---

### 2. LSTM 公式

符号定义：

* 输入：x\_t
* 隐状态：h\_t
* 细胞状态：c\_t

公式：

1. 遗忘门

   ```
   f_t = sigmoid(W_f * [h_{t-1}, x_t] + b_f)
   ```
2. 输入门

   ```
   i_t = sigmoid(W_i * [h_{t-1}, x_t] + b_i)
   Ĉ_t = tanh(W_c * [h_{t-1}, x_t] + b_c)
   ```
3. 更新细胞状态

   ```
   c_t = f_t * c_{t-1} + i_t * Ĉ_t
   ```
4. 输出门

   ```
   o_t = sigmoid(W_o * [h_{t-1}, x_t] + b_o)
   h_t = o_t * tanh(c_t)
   ```

---
### LSTM解决了RNN的什么问题？为什么？
答：LSTM解决了传统RNN中的梯度消失和梯度爆炸问题。
传统的RNN在处理长序列数据时存在梯度消失或梯度爆炸问题，主要原因是在反向传播过程中，梯度会随着时间步的增加而指数级地增大或减小。这导致在训练过程中，网络在处理长序列时很难有效传播梯度，导致长期依赖关系难以捕捉。LSTM通过引入门控机制解决了传统RNN中的梯度消失和梯度爆炸问题。
### 3. 为什么 LSTM 同时用 sigmoid 和 tanh？

* sigmoid：控制门控（保留/遗忘 → 0\~1 概率）
* tanh：处理状态值（平衡梯度范围 -1\~1）

---

## 六、GRU和LSTM的区别（门控循环单元）

* **简化版 LSTM**
* 门控结构：仅有 **重置门、更新门**
* 参数更少 → 计算更快
* 性能与 LSTM 接近，在部分任务上更高效


GRU和LSTM都是用于解决长期依赖问题的改进型循环神经网络（RNN）结构。它们在解决长期依赖问题上有相似之处，但也有一些不同点。以下是它们之间的主要区别：
1. 门控结构不同：
    * LSTM具有三个门控：输入门（Input Gate）、遗忘门（Forget Gate）和输出门（Output Gate）。这三个门控通过sigmoid激活函数来控制信息的流动和细胞状态的更新。
    * GRU具有两个门控：重置门（Reset Gate）和更新门（Update Gate）。重置门决定了过去信息在当前时间步的输入中有多少保留，而更新门决定了过去和当前信息在当前时间步的更新中所占的比例。
2. 参数数量不同：
    * GRU的参数较少，因为它合并了LSTM的细胞状态和隐藏状态，并减少了一个门控。这使得GRU在计算上相对更轻量级。
    * LSTM的参数较多，因为它有三个门控，每个门控都有自己的权重和偏置。
3. 细胞状态更新不同：
    * LSTM的细胞状态由遗忘门、输入门和输出门三个门控决定，分别控制了细胞状态的遗忘、增益和输出。
    * GRU的细胞状态由重置门和更新门两个门控决定，重置门控制了过去信息的保留，更新门控制了过去和当前信息的更新。
4. 计算效率不同：
    * 由于参数较少，GRU的计算效率通常高于LSTM，特别是在较大规模的模型或者大规模数据上。
5. 表现情况不同：
    * 在一些任务上，LSTM和GRU的表现可能会有所不同。在某些序列建模任务中，LSTM可能更适合处理长期依赖问题，而在另一些任务中，GRU可能表现得更好。
总的来说，LSTM和GRU都是强大的RNN变种，用于处理序列数据和解决长期依赖问题。

---

## 七、RNN / LSTM / GRU 的对比

| 特点   | RNN     | LSTM               | GRU       |
| ---- | ------- | ------------------ | --------- |
| 梯度问题 | 容易消失/爆炸 | 门控机制缓解，能处理长期依赖     | 简化门控，参数更少 |
| 参数量  | 少       | 多（三个门控）            | 较少（两个门控）  |
| 计算效率 | 高       | 较低                 | 较高        |
| 适用任务 | 短序列     | 长序列、复杂任务（NLP、时间序列） | 中长序列，效率优先 |

---

## 八、如何衡量语言模型好坏？

常见指标：

1. **困惑度（Perplexity）**
2. **交叉熵损失（Cross-Entropy Loss）**
3. **下游任务性能**（准确率、BLEU、ROUGE、F1 等）
4. **生成质量**（流畅性、一致性、合理性）
5. **效率**（训练时间、推理速度、参数量）


